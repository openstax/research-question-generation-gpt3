# additional clarifications

Here we provide some additional clarifications to our paper. Some of them address concerns raised by reviewers that do not fit in the space allowed in the camera-ready version of the paper.

1. **It would also help to have specific examples within each question.**

Please see the file `generated_questions_examples` for examples of textbook contexts and the generated questions.

2. **(In the human evaluation) the author selected the 10 "best" questions from the ones generated, which could be a pretty strong selection bias.**

Because question generation models can sometimes generate unreliable questions, in the human evaluation in this paper, we focus on the "best-case scenario", which examines the best possible questions that the model can generate. We choose the "best" questions ourselves; In this sense, the selection is biased by human judgments. It is our on-going work to design an automated, human-in-the-loop selection mechanism such that it can assist humans in choosing reliable, high-quality, and pedagogically valuable questions from those generated by the model.

3. **The type of question and subject matter is not described. It suggests capability for any subject matter.**

We primarily focus on open-ended types of question of Bloom's level 1 and 2. We have tested on generating higher Bloom's level questions; In general, the model is better at generating lower Bloom's level questions. We have also tested on True-False, multiple-choice, and fill-in-the-blank types of questions; the observations are similar to those of the open-ended questions that we have reported in the paper but we did not conduct a comprehensive study. It is an interesting future work to study how the model performs when generating questions of different types or of different difficulty level; see the section "Limitations and Future Work" in the paper for a short discussion.

@Debshila Subject matter details 

4. **Statistics from 3.1 and 3.3 seem not to match. Presumably 52% vs. over 70%, these differences are not explained.**

These two sections describe different experiments. Section 3.1 exmaines the generated questions without selection; Section 3.3 examines the "best" generated questions which are selected. Therefore, it is expected that the results in Seciton 3.3 should be better than Section 3.1 and our results indeed show so.

5. **More details on Equation 1 and 2.**

notation: a *token* is the smallest unit in a piece of text, e.g., a word, that is processed by the model.

Equation 1 represents a standard auto-regressive generative model. Intuitively, it generates the output text, which is a question in our case, one token at a time from the beginning to the end, until the entire question is generated.

Equation 2 further conditions the generation on a "prompt", a set of examples that guide the model to generate a specific text, which is a quetsion in our case. These prompts in our case are examples of "context-question" pairs that we use to instruct the model to generate questions rather than any other arbitrary texts.

6. **The configurations of other factors when examining the variations of a certain factor.**

| Factor under examination |                   |             | Fixed factors |                |                 |
|--------------------------|:-----------------:|:-----------:|:-------------:|:--------------:|:---------------:|
|                          | Example structure | Data source | Num. examples | Context length | Question length |
| Example structure        |         -         | Openstax    | 0,1,2,3       | Small          | Small           |
| Data source              | CTQA              |      -      | 0,1,2,3       | Small          | Small           |
| Num. examples            | CTQA              | Openstax    |       -       | Small          | Small           |
| Context length           | CTQA              | Openstax    | 3,5,7         |        -       | Small           |
| Question length          | CTQA              | Openstax    | 3,5,7         | Medium         |        -        |

7. **In Section 2.1, how a target was determined.**

We collect the target either from the glossary of a given textbook or from those questions at the end of a chapter in a given textbook.
